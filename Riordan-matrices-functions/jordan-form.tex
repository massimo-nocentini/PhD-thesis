
We begin this section with necessary definitions about Jordan canonical forms
to help the computation of matrices functions.

Let $A\in\mathbb{R}^{m\times m}$ be a square matrix and $\Phi_{i,j}
\in\prod_{m-1}$ a generalized Lagrange base; according to \cite{LT2002},
$Z_{i,j}^{[A]} = \Phi_{i,j}(A)$ is a \textit{component matrix} of $A$ (from
here on, we just write $Z_{i,j}$ to keep clean the notation when no confusion
arises). Let $\boldsymbol{v}\in\mathbb{R}^{m}$ be a \textit{non-zero} vector to
define a set of subspaces
\begin{displaymath}
\mathcal{M}_{i} = \left\lbrace \boldsymbol{x}_{i,j} = Z_{i,2}^{j-1}\,Z_{i,1}\,\boldsymbol{v},\,j\in\lbrace1,\ldots,m_{i}\rbrace\right\rbrace, \quad i\in \lbrace 1,\ldots,\nu \rbrace,
\end{displaymath}
where $dim(\mathcal{M}_{i})=m_{i}$; moreover, vectors $\boldsymbol{x}_{i,j}$ are
linearly independent, therefore $\mathcal{M}_{q}\cap\mathcal{M}_{w}=\emptyset$ if $q\neq w$.
\begin{lemma}
Let $\lambda_{i}\in\sigma(A)$, then vectors
$\boldsymbol{x}_{i,j}\in\mathcal{M}_{i}$ satisfy the recurrence relation
\begin{displaymath}
\begin{split}
A\,\boldsymbol{x}_{i,j} &= \lambda_{i}\,\boldsymbol{x}_{i,j} + \boldsymbol{x}_{i,j+1} , \quad j\in \lbrace 1,\ldots,m_{i}-1 \rbrace  \\
A\,\boldsymbol{x}_{i,m_{i}} &= \lambda_{i}\,\boldsymbol{x}_{i,m_{i}} \\
\end{split}
\end{displaymath}
\end{lemma}
\begin{proof}
Component matrices commute with respect to matrix product, namely
$Z_{ij}Z_{kr}= Z_{kr}Z_{ij}$; moreover, identities $Z_{i2} = Z_{i1}(A-\lambda_{i}I)$,
$Z_{i,2}^{m_{i}}=O$ and $Z_{i1}Z_{ij}=Z_{ij}$ also hold, so
\begin{displaymath}
\begin{split}
\boldsymbol{x}_{i,j+1} &= Z_{i,2}^{j}\,Z_{i,1}\,\boldsymbol{v} = Z_{i,2}\,Z_{i,2}^{j-1}\,Z_{i,1}\,\boldsymbol{v} =  Z_{i,2}\,\boldsymbol{x}_{i,j}=A\,\boldsymbol{x}_{i,j} - \lambda_{i}\,\boldsymbol{x}_{i,j}, \quad j\in \lbrace 1,\ldots,m_{i}-1 \rbrace  \\
Z_{i,2}\,\boldsymbol{x}_{i,m_{i}} &=  Z_{i,2}\,Z_{i,2}^{m_{i}-1}\,Z_{i,1}\,\boldsymbol{v} = Z_{i,2}^{m_{i}}\,Z_{i,1}\,\boldsymbol{v} = \boldsymbol{0}
\end{split}
\end{displaymath}
Proofs of component matrices's properties can be found in \cite{BT1998, LT2002}.
\end{proof}
The recurrence relation can be rewritten in matrix notation as $A\,X_{i} = X_{i}\,J_{i}$ where
\begin{displaymath}
X_{i} = \left[\boldsymbol{x}_{i,1},\ldots,\boldsymbol{x}_{i,m_{i}} \right]\in\mathbb{R}^{m\times m_{i}} \quad\quad
J_{i} = \left[ \begin{array}{cccc}
    \lambda_{i} \\
    1 & \lambda_{i} \\
      & \ddots & \ddots \\
      & & 1 &\lambda_{i} \\
\end{array} \right] \in\mathbb{R}^{m_{i}\times m_{i}}
\end{displaymath}
Under this point of view, vectors $\boldsymbol{x}_{i,j}\in\mathcal{M}_{i}$ are
called \textit{generalized eigenvectors} ($\boldsymbol{x}_{i,m_{i}}$ is an
eigenvector, as usual) relative to $A$'s eigenvalue $\lambda_{i}$; at last,
$J_{i}$ is called \textit{Jordan block}.  Collecting matrices $X_{i}$ and
$J_{i}$ for $i\in \lbrace 1,\ldots,\nu \rbrace$, the \textit{Jordan canonical
form} of $A$ is defined by the relation $A\,X = X\, J$, where
\begin{displaymath}
X = \left[X_{1},\ldots,X_{\nu} \right]\in\mathbb{R}^{m\times m} \quad\quad
J = \left[ \begin{array}{ccc}
    J_{1} \\
      & \ddots \\
      & & J_{\nu} \\
\end{array} \right] \in\mathbb{R}^{m\times m}
\end{displaymath}
with respect to vector $\boldsymbol{v}\in\mathbb{R}^{m}$; finally, if $X$ is
non-singular then matrices $A$ and $X^{-1}\,A\,X = J$ are \textit{similar}, $A
\sim_{X} J$ in symbols. This derivations allow us to compute functions of
matrices in a easier way, with the help of the following 
\begin{lemma} Let $f$ be a function defined on $\sigma(A)$ and $g$ the 
corresponding Hermite interpolating polynomial. Then $ A \sim_{X} J \rightarrow
g(A) \sim_{X} g(J) $, for a matrix $X$ which depends on a arbitrary vector
$\boldsymbol{v}\in\mathbb{R}^{m}$.
\end{lemma}
\begin{proof}
By definition of similarity relation $ X^{-1}\,A\,X = J$, application of $g$ to
both members preserves the identity $ g(X^{-1}\,A\,X) = g(J)$; finally, since
$g$ is a linear combination of powers being a polynomial,
$\left(X^{-1}\,A\,X\right)^{i} = X^{-1}\,A^{i}\,X$ entails $X^{-1}\,g(A)\,X =
g(J)$, as required.
\end{proof}
Previous lemma ensures that $A \sim_{X} J\rightarrow g(A) = X\,g(J)\,X^{-1}$
and allows us to compute $f(A)$: in words, the procedure consists of, first,
finding matrices $X$ and $J$; second, compute $g(J)$; third, multiply it by $X$
on the left side and by $X^{-1}$ on the right side. Now, to study the
application of $f$ to $J$ we can focus on the application of $f$ to the Jordan
block $J_{i}$ due to the block-wise structure of matrix $J$ and, lately,
compose results block-wise as well. 
\iffalse % \begin{displaymath} {{{
f(J) = \left[ \begin{array}{ccc}
        f(J_{1}) \\
        & \ddots \\
        & & f(J_{\nu}) \\
\end{array} \right] \in\mathbb{R}^{m\times m}
\end{displaymath}
\fi
% }}}

\begin{remark}
Since the Jordan block $J_{i}$ is a $m_{i}$-minor of the Riordan array $\left(\lambda_{i}+t,
t\right)$ then it shares the same base of polynomials shown in
\autoref{eq:generalized-Lagrange-polynomials-RA}, hence for a function $f$
defined on $\sigma(J_{i})$, the application $f(J_{i})$ yields
\begin{displaymath}
\small
f{\left (J_{i} \right )} = \left[\begin{matrix}f{\left (\lambda_{i} \right )} &  &  &  &  &  &  & \\\frac{d}{d \lambda_{i}} f{\left (\lambda_{i} \right )} & f{\left (\lambda_{i} \right )} &  &  &  &  &  & \\\frac{1}{2} \frac{d^{2}}{d \lambda_{i}^{2}}  f{\left (\lambda_{i} \right )} & \frac{d}{d \lambda_{i}} f{\left (\lambda_{i} \right )} & f{\left (\lambda_{i} \right )} &  &  &  &  & \\\frac{1}{6} \frac{d^{3}}{d \lambda_{i}^{3}}  f{\left (\lambda_{i} \right )} & \frac{1}{2} \frac{d^{2}}{d \lambda_{i}^{2}}  f{\left (\lambda_{i} \right )} & \frac{d}{d \lambda_{i}} f{\left (\lambda_{i} \right )} & f{\left (\lambda_{i} \right )} &  &  &  & \\\frac{1}{24} \frac{d^{4}}{d \lambda_{i}^{4}}  f{\left (\lambda_{i} \right )} & \frac{1}{6} \frac{d^{3}}{d \lambda_{i}^{3}}  f{\left (\lambda_{i} \right )} & \frac{1}{2} \frac{d^{2}}{d \lambda_{i}^{2}}  f{\left (\lambda_{i} \right )} & \frac{d}{d \lambda_{i}} f{\left (\lambda_{i} \right )} & f{\left (\lambda_{i} \right )} &  &  & \\\frac{1}{120} \frac{d^{5}}{d \lambda_{i}^{5}}  f{\left (\lambda_{i} \right )} & \frac{1}{24} \frac{d^{4}}{d \lambda_{i}^{4}}  f{\left (\lambda_{i} \right )} & \frac{1}{6} \frac{d^{3}}{d \lambda_{i}^{3}}  f{\left (\lambda_{i} \right )} & \frac{1}{2} \frac{d^{2}}{d \lambda_{i}^{2}}  f{\left (\lambda_{i} \right )} & \frac{d}{d \lambda_{i}} f{\left (\lambda_{i} \right )} & f{\left (\lambda_{i} \right )} &  & \\ \vdots &  \vdots &  \vdots &  \vdots &  \vdots &  \vdots & \ddots & \\\frac{1}{(m_{i}-1)!} \frac{d^{m_{i}-1}}{d \lambda_{i}^{m_{i}-1}}  f{\left (\lambda_{i} \right )} & \frac{1}{(m_{i}-2)!} \frac{d^{m_{i}-2}}{d \lambda_{i}^{m_{i}-2}}  f{\left (\lambda_{i} \right )} & \ldots & \ldots & \ldots & \ldots & \frac{d}{d \lambda_{i}} f{\left (\lambda_{i} \right )} & f{\left (\lambda_{i} \right )}\end{matrix}\right].
\end{displaymath}
\end{remark}

We show columns for the family of functions studied in previous sections
for a minor $8\times8$:
\begin{displaymath}
\begin{split}
J_{i}^{r} \boldsymbol{e}_{1} &= \left[\begin{matrix}\frac{{\left(r\right)}_{1} \lambda_{i}^{r}}{0!}\\\frac{{\left(r\right)}_{i}}{1!} \lambda_{i}^{r - 1}\\\frac{{\left(r\right)}_{2}}{2!} \lambda_{i}^{r - 2}\\\frac{{\left(r\right)}_{3}}{3!} \lambda_{i}^{r - 3}\\\frac{{\left(r\right)}_{4}}{4!} \lambda_{i}^{r - 4}\\\frac{{\left(r\right)}_{5}}{5!} \lambda_{i}^{r - 5}\\\frac{{\left(r\right)}_{6}}{6!} \lambda_{i}^{r - 6}\\\frac{{\left(r\right)}_{7}}{7!} \lambda_{i}^{r - 7}\end{matrix}\right],\quad
\frac{\boldsymbol{e}_{1}}{J_{i}} = \left[\begin{matrix}\frac{1}{\lambda_{i}}\\- \frac{1}{\lambda_{i}^{2}}\\\frac{1}{\lambda_{i}^{3}}\\- \frac{1}{\lambda_{i}^{4}}\\\frac{1}{\lambda_{i}^{5}}\\- \frac{1}{\lambda_{i}^{6}}\\\frac{1}{\lambda_{i}^{7}}\\- \frac{1}{\lambda_{i}^{8}}\end{matrix}\right],\quad
\sqrt{J_{i}} \boldsymbol{e}_{1} = \left[\begin{matrix}\sqrt{\lambda_{i}}\\\frac{1}{2 \sqrt{\lambda_{i}}}\\- \frac{1}{8 \lambda_{i}^{\frac{3}{2}}}\\\frac{1}{16 \lambda_{i}^{\frac{5}{2}}}\\- \frac{5}{128 \lambda_{i}^{\frac{7}{2}}}\\\frac{7}{256 \lambda_{i}^{\frac{9}{2}}}\\- \frac{21}{1024 \lambda_{i}^{\frac{11}{2}}}\\\frac{33}{2048 \lambda_{i}^{\frac{13}{2}}}\end{matrix}\right], \quad
e^{J_{i} \alpha} \boldsymbol{e}_{1} = \left[\begin{matrix}e^{\alpha \lambda_{i}}\\\alpha e^{\alpha \lambda_{i}}\\\frac{\alpha^{2}}{2} e^{\alpha \lambda_{i}}\\\frac{\alpha^{3}}{6} e^{\alpha \lambda_{i}}\\\frac{\alpha^{4}}{24} e^{\alpha \lambda_{i}}\\\frac{\alpha^{5}}{120} e^{\alpha \lambda_{i}}\\\frac{\alpha^{6}}{720} e^{\alpha \lambda_{i}}\\\frac{\alpha^{7}}{5040} e^{\alpha \lambda_{i}}\end{matrix}\right], \\
\log{\left (J_{i} \right )} \boldsymbol{e}_{1} &= \left[\begin{matrix}\log{\left (\lambda_{i} \right )}\\\frac{1}{\lambda_{i}}\\- \frac{1}{2 \lambda_{i}^{2}}\\\frac{1}{3 \lambda_{i}^{3}}\\- \frac{1}{4 \lambda_{i}^{4}}\\\frac{1}{5 \lambda_{i}^{5}}\\- \frac{1}{6 \lambda_{i}^{6}}\\\frac{1}{7 \lambda_{i}^{7}}\end{matrix}\right], \quad
\sin{\left (J_{i} \right )} \boldsymbol{e}_{1} = \left[\begin{matrix}\sin{\left (\lambda_{i} \right )}\\\cos{\left (\lambda_{i} \right )}\\- \frac{1}{2} \sin{\left (\lambda_{i} \right )}\\- \frac{1}{6} \cos{\left (\lambda_{i} \right )}\\\frac{1}{24} \sin{\left (\lambda_{i} \right )}\\\frac{1}{120} \cos{\left (\lambda_{i} \right )}\\- \frac{1}{720} \sin{\left (\lambda_{i} \right )}\\- \frac{1}{5040} \cos{\left (\lambda_{i} \right )}\end{matrix}\right]
\quad\text{and}\quad
\cos{\left (J_{i} \right )} \boldsymbol{e}_{1} = \left[\begin{matrix}\cos{\left (\lambda_{i} \right )}\\- \sin{\left (\lambda_{i} \right )}\\- \frac{1}{2} \cos{\left (\lambda_{i} \right )}\\\frac{1}{6} \sin{\left (\lambda_{i} \right )}\\\frac{1}{24} \cos{\left (\lambda_{i} \right )}\\- \frac{1}{120} \sin{\left (\lambda_{i} \right )}\\- \frac{1}{720} \cos{\left (\lambda_{i} \right )}\\\frac{1}{5040} \sin{\left (\lambda_{i} \right )}\end{matrix}\right]; \quad
\end{split}
\end{displaymath}
moreover, observe that if $A$ is a Riordan array then its Jordan canonical form
reduces to matrices $X = X_{1}$ and $J = J_{1}$ because of the unique
eigenvalue $\lambda_{1}$ of algebraic multiplicity $m_{1} = m$.

\begin{example}
Let $\mathcal{P}\sim_{X}J$, then Pascal triangle's inverse 
$\mathcal{P}^{-1}$ can be computed by 
\iffalse % \begin{displaymath} {{{
\scriptsize
\left[\begin{matrix}\frac{1}{\lambda_{1}} & 0 & 0 & 0 & 0 & 0 & 0 & 0\\- \frac{1}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}} & 0 & 0 & 0 & 0 & 0 & 0\\- \frac{1}{\lambda_{1}^{2}} + \frac{2}{\lambda_{1}^{3}} & - \frac{2}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}} & 0 & 0 & 0 & 0 & 0\\- \frac{1}{\lambda_{1}^{2}} + \frac{6}{\lambda_{1}^{3}} - \frac{6}{\lambda_{1}^{4}} & - \frac{3}{\lambda_{1}^{2}} + \frac{6}{\lambda_{1}^{3}} & - \frac{3}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}} & 0 & 0 & 0 & 0\\- \frac{1}{\lambda_{1}^{2}} + \frac{14}{\lambda_{1}^{3}} - \frac{36}{\lambda_{1}^{4}} + \frac{24}{\lambda_{1}^{5}} & - \frac{4}{\lambda_{1}^{2}} + \frac{24}{\lambda_{1}^{3}} - \frac{24}{\lambda_{1}^{4}} & - \frac{6}{\lambda_{1}^{2}} + \frac{12}{\lambda_{1}^{3}} & - \frac{4}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}} & 0 & 0 & 0\\- \frac{1}{\lambda_{1}^{2}} + \frac{30}{\lambda_{1}^{3}} - \frac{150}{\lambda_{1}^{4}} + \frac{240}{\lambda_{1}^{5}} - \frac{120}{\lambda_{1}^{6}} & - \frac{5}{\lambda_{1}^{2}} + \frac{70}{\lambda_{1}^{3}} - \frac{180}{\lambda_{1}^{4}} + \frac{120}{\lambda_{1}^{5}} & - \frac{10}{\lambda_{1}^{2}} + \frac{60}{\lambda_{1}^{3}} - \frac{60}{\lambda_{1}^{4}} & - \frac{10}{\lambda_{1}^{2}} + \frac{20}{\lambda_{1}^{3}} & - \frac{5}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}} & 0 & 0\\- \frac{1}{\lambda_{1}^{2}} + \frac{62}{\lambda_{1}^{3}} - \frac{540}{\lambda_{1}^{4}} + \frac{1560}{\lambda_{1}^{5}} - \frac{1800}{\lambda_{1}^{6}} + \frac{720}{\lambda_{1}^{7}} & - \frac{6}{\lambda_{1}^{2}} + \frac{180}{\lambda_{1}^{3}} - \frac{900}{\lambda_{1}^{4}} + \frac{1440}{\lambda_{1}^{5}} - \frac{720}{\lambda_{1}^{6}} & - \frac{15}{\lambda_{1}^{2}} + \frac{210}{\lambda_{1}^{3}} - \frac{540}{\lambda_{1}^{4}} + \frac{360}{\lambda_{1}^{5}} & - \frac{20}{\lambda_{1}^{2}} + \frac{120}{\lambda_{1}^{3}} - \frac{120}{\lambda_{1}^{4}} & - \frac{15}{\lambda_{1}^{2}} + \frac{30}{\lambda_{1}^{3}} & - \frac{6}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}} & 0\\- \frac{1}{\lambda_{1}^{2}} + \frac{126}{\lambda_{1}^{3}} - \frac{1806}{\lambda_{1}^{4}} + \frac{8400}{\lambda_{1}^{5}} - \frac{16800}{\lambda_{1}^{6}} + \frac{15120}{\lambda_{1}^{7}} - \frac{5040}{\lambda_{1}^{8}} & - \frac{7}{\lambda_{1}^{2}} + \frac{434}{\lambda_{1}^{3}} - \frac{3780}{\lambda_{1}^{4}} + \frac{10920}{\lambda_{1}^{5}} - \frac{12600}{\lambda_{1}^{6}} + \frac{5040}{\lambda_{1}^{7}} & - \frac{21}{\lambda_{1}^{2}} + \frac{630}{\lambda_{1}^{3}} - \frac{3150}{\lambda_{1}^{4}} + \frac{5040}{\lambda_{1}^{5}} - \frac{2520}{\lambda_{1}^{6}} & - \frac{35}{\lambda_{1}^{2}} + \frac{490}{\lambda_{1}^{3}} - \frac{1260}{\lambda_{1}^{4}} + \frac{840}{\lambda_{1}^{5}} & - \frac{35}{\lambda_{1}^{2}} + \frac{210}{\lambda_{1}^{3}} - \frac{210}{\lambda_{1}^{4}} & - \frac{21}{\lambda_{1}^{2}} + \frac{42}{\lambda_{1}^{3}} & - \frac{7}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}}\end{matrix}\right]
\end{displaymath}
\fi
% }}}
$\mathcal{P}^{-1} = X\,J^{-1}\,X^{-1}$, where
\begin{displaymath}
X = \alpha_{0} \left[\begin{matrix}1 &  &  &  &  &  &  & \\0 & 1 &  &  &  &  &  & \\0 & 1 & 2 &  &  &  &  & \\0 & 1 & 6 & 6 &  &  &  & \\0 & 1 & 14 & 36 & 24 &  &  & \\0 & 1 & 30 & 150 & 240 & 120 &  & \\0 & 1 & 62 & 540 & 1560 & 1800 & 720 & \\0 & 1 & 126 & 1806 & 8400 & 16800 & 15120 & 5040\end{matrix}\right]\,
\text{depends on}\,\, \boldsymbol{v}= \left[\begin{matrix} \alpha_{0}\\0\\0\\0\\0\\0\\0\\0 \end{matrix}\right],\,\alpha_{0}\in\mathbb{R};
\end{displaymath}
for completeness,
    \autoref{subsec:Pascal-component-matrices-generalized-eigenvectors}
    contains $\mathcal{P}_{8}$'s component matrices and its generalized
    eigenvectors.
\end{example}
\iffalse % with $\boldsymbol{\alpha} = \left[ \alpha_{0}, 0,0,0,0,0,0,0 \right]^{T}$, and {{{
\begin{displaymath}
J^{-1} = \left[\begin{matrix}\frac{1}{\lambda_{1}} & 0 & 0 & 0 & 0 & 0 & 0 & 0\\- \frac{1}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}} & 0 & 0 & 0 & 0 & 0 & 0\\\frac{1}{\lambda_{1}^{3}} & - \frac{1}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}} & 0 & 0 & 0 & 0 & 0\\- \frac{1}{\lambda_{1}^{4}} & \frac{1}{\lambda_{1}^{3}} & - \frac{1}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}} & 0 & 0 & 0 & 0\\\frac{1}{\lambda_{1}^{5}} & - \frac{1}{\lambda_{1}^{4}} & \frac{1}{\lambda_{1}^{3}} & - \frac{1}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}} & 0 & 0 & 0\\- \frac{1}{\lambda_{1}^{6}} & \frac{1}{\lambda_{1}^{5}} & - \frac{1}{\lambda_{1}^{4}} & \frac{1}{\lambda_{1}^{3}} & - \frac{1}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}} & 0 & 0\\\frac{1}{\lambda_{1}^{7}} & - \frac{1}{\lambda_{1}^{6}} & \frac{1}{\lambda_{1}^{5}} & - \frac{1}{\lambda_{1}^{4}} & \frac{1}{\lambda_{1}^{3}} & - \frac{1}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}} & 0\\- \frac{1}{\lambda_{1}^{8}} & \frac{1}{\lambda_{1}^{7}} & - \frac{1}{\lambda_{1}^{6}} & \frac{1}{\lambda_{1}^{5}} & - \frac{1}{\lambda_{1}^{4}} & \frac{1}{\lambda_{1}^{3}} & - \frac{1}{\lambda_{1}^{2}} & \frac{1}{\lambda_{1}}\end{matrix}\right]
\end{displaymath}
\fi
% }}}
The following theorem uses the property that any two Riordan arrays
share the same matrix $J$ in their Jordan canonical forms to state that they
are combination the one of the other.
\begin{theorem}
Let $A$ and $B$ be two Riordan matrices and let $A\,X = X\,J$ and $B\,Y= Y\,J$
be their Jordan canonical forms, respectively, where matrices $X$ and $Y$ depend
on complex vectors $\boldsymbol{v}$ and $\boldsymbol{w}$; then, $A
\sim_{X\,Y^{-1}} B$. Moreover, $f(A) \sim_{X\,Y^{-1}} f(B)$ also holds, for any
function $f$ defined on $\sigma(A)$.
\end{theorem}
\begin{proof}
By transitivity of the similarity relation, $X^{-1}\,A\,X = Y^{-1}\,B\,Y$
entails $Y\,X^{-1}\,A\,X\,Y^{-1} = B$. Finally, let $g$ be the Hermite
interpolating polynomial of $f$, then $g(Y\,X^{-1}\,A\,X\,Y^{-1}) = g(B)$
implies $Y\,X^{-1}\,g(A)\,X\,Y^{-1} = g(B)$, as required.
\end{proof}

\begin{example}
Pascal and Catalan triangles are similar with respect to
$\mathcal{P} \sim_{X\,Y^{-1}}\mathcal{C}$ and $\mathcal{C}
\sim_{Y\,X^{-1}}\mathcal{P}$, where 
\begin{displaymath}
Y = \beta_{0} \left[\begin{matrix}1 &  &  &  &  &  &  & \\0 & 1 &  &  &  &  &  & \\0 & 2 & 2 &  &  &  &  & \\0 & 5 & 11 & 6 &  &  &  & \\0 & 14 & 52 & 62 & 24 &  &  & \\0 & 42 & 238 & 470 & 394 & 120 &  & \\0 & 132 & 1084 & 3176 & 4348 & 2844 & 720 & \\0 & 429 & 4956 & 20323 & 40562 & 42874 & 23148 & 5040\end{matrix}\right]
\,\,\text{depends on}\,\,\boldsymbol{w}= \left[\begin{matrix} \beta_{0}\\0\\0\\0\\0\\0\\0\\0 \end{matrix}\right],\,\beta_{0}\in\mathbb{R}.
\end{displaymath}
given $\mathcal{C}\sim_{Y}J$ and $\mathcal{P}\sim_{X}J$, as before.
\end{example}


\iffalse % \begin{displaymath} {{{
{X_{\boldsymbol{\alpha}}\,\left(Y_{\boldsymbol{\beta}}\right)^{-1}} = \frac{\alpha_{0}}{\beta_{0}} \left[\begin{matrix}1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\0 & -1 & 1 & 0 & 0 & 0 & 0 & 0\\0 & 1 & - \frac{5}{2} & 1 & 0 & 0 & 0 & 0\\0 & -1 & \frac{29}{6} & - \frac{13}{3} & 1 & 0 & 0 & 0\\0 & 1 & - \frac{613}{72} & \frac{467}{36} & - \frac{77}{12} & 1 & 0 & 0\\0 & -1 & \frac{10331}{720} & - \frac{11989}{360} & \frac{3199}{120} & - \frac{87}{10} & 1 & 0\\0 & 1 & - \frac{1019899}{43200} & \frac{1701701}{21600} & - \frac{656591}{7200} & \frac{28183}{600} & - \frac{223}{20} & 1\end{matrix}\right]
\end{displaymath}
On the other hand,
$\mathcal{C} \sim_{Y_{\boldsymbol{\beta}}\,\left(X_{\boldsymbol{\alpha}}\right)^{-1}}\mathcal{P}$, where
\begin{displaymath}
{Y_{\boldsymbol{\beta}}\,\left(X_{\boldsymbol{\alpha}}\right)^{-1}} = \frac{\beta_{0}}{\alpha_{0}} \left[\begin{matrix}1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\0 & 1 & 1 & 0 & 0 & 0 & 0 & 0\\0 & \frac{3}{2} & \frac{5}{2} & 1 & 0 & 0 & 0 & 0\\0 & \frac{8}{3} & 6 & \frac{13}{3} & 1 & 0 & 0 & 0\\0 & \frac{31}{6} & \frac{175}{12} & \frac{89}{6} & \frac{77}{12} & 1 & 0 & 0\\0 & \frac{157}{15} & \frac{215}{6} & \frac{281}{6} & \frac{175}{6} & \frac{87}{10} & 1 & 0\\0 & \frac{649}{30} & \frac{1767}{20} & \frac{851}{6} & 115 & \frac{1501}{30} & \frac{223}{20} & 1\end{matrix}\right]
\end{displaymath}
as required. 
\fi
% }}}

Finally, since the product of a Riordan matrix $\mathcal{R}\left(d(t),
h(t)\right)$ and an infinite vector $\boldsymbol{b}=(b_{i})_{i\in\mathbb{N}}$,
where $b(t) = \sum_{i\in\mathbb{N}}{b_{i}t^{i}}$, yields
$\mathcal{R}\cdot\boldsymbol{b} = d(t)b(h(t))$ by the fundamental theorem of
Riordan arrays, in the next theorem we show a connection to this result.

\begin{theorem}
Let $A$ be a Riordan matrix, $\boldsymbol{b}$ a vector and $A\,X = X\,J$ be the
$A$'s Jordan canonical form built on matrices $J$ and $X$ depending on
$\boldsymbol{b}$.  Let $f$ be a function  defined on $\sigma(A)$, then
$f(A)\cdot\boldsymbol{b} = X\,f(J)\,\boldsymbol{e}_{0}$. 
\end{theorem}
\begin{proof}
Observe that
$\left(X_{\boldsymbol{b}}\right)^{-1}\,\boldsymbol{b}=\boldsymbol{e}_{0}$ holds
because $X_{\boldsymbol{b}}\,\boldsymbol{e}_{0}=\boldsymbol{x}_{1,1} =
Z_{1,2}^{0}\,Z_{1,1}\boldsymbol{b}=\boldsymbol{b}$.  Let $g$ be the Hermite
interpolating polynomial of function $f$, then $f(A) = X\,g(J)\,X^{-1}$ entails
$f(A)\cdot\boldsymbol{b} = X\,g(J)\,X^{-1}\cdot\boldsymbol{b}$, provided that
$X$ depends on $\boldsymbol{b}$.
\end{proof}
